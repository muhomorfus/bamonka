\chapter{Методы решения задачи распределения ресурсов в вычислительном кластере}

В общем случае, задачу распределения ресурсов в вычислительном кластере можно описать так: необходимо выбрать, какой рабочий узел будет выполнять условное задание. В основном, под рабочим узлом подразумевается физический сервер, а под заданием~---~приложение~\cite{bittencourt2018scheduling}.

Задача распределения ресурсов в вычислительном кластере является NP-трудной~\cite{mor2021heuristic}\cite{guo2020cloud} и не может быть решена за полиномиальное время~\cite{marrouche2024unlocking}.

Для задачи планирования ресурсов традиционным решением являются эвристические алгоритмы~\cite{marrouche2024unlocking}. Эвристические алгоритмы нацелены на универсальность, а также легко понимаются и реализуются. Примеры таких алгоритмов включают «Round-Robin»~\cite{ghazy2023ameliorated}, «First Fit»~\cite{keshri2023communication} и «Max-Min»~\cite{raeisi2024advanced}, которые хорошо работают при определённых нагрузках. Однако эвристические алгоритмы обычно используют фиксированные параметры и политики планирования. Они не могут адаптироваться к изменяющейся среде, из-за чего не достигают оптимальной производительности. 

Некоторые исследования предлагают использовать метаэвристические алгоритмы для улучшения эвристических подходов. Эти алгоритмы комбинируют эвристику с рандомизированными алгоритмами и алгоритмами локального поиска, такими как алгоритм муравьиной колонии, алгоритм имитации отжига и генетический алгоритм~\cite{mishra2024metaheuristic}. Однако разработка этих метаэвристик является сложной и трудоёмкой задачей. Как правило, сначала разрабатывается простой эвристический алгоритм в соответствии с данным сценарием планирования. Затем параметры эвристического алгоритма пытаются настроить вручную в зависимости от характеристик приложений и целей планирования. Этот процесс требует многократного ручного тестирования и настройки, а также наличия экспертных знаний у разработчика о состоянии кластера и его рабочей нагрузке~\cite{jian2024drs}.

\section{Архитектура Kubernetes}

Kubernetes~---~система для запуска контейнеризированных приложений (оркестратор) с открытым исходным кодом, позволяет управлять жизненным циклом работы контейнеров: запускать, удалять, перезапускать в случае возникновения ошибок~\cite{kubernetes}. При этом пользователь абстрагирован от внутренней логики выполнения приложений, такой как расположение контейнера на каком-то из физических узлов кластера.

Основным понятием в Kubernetes является ресурс. Ресурс~---~описание некоторого типа объекта в Kubernetes. Все понятия, с которыми оперирует оркестратор, представлены в виде объектов, и хранятся в базе данных~\cite{kubernetesresources}. Для того, чтобы избежать коллизии понятий, в для задачи распределения ресурсов вычислительном кластере будет использоваться термин «задача планирования запуска приложений в кластере».

Kubernetes оперирует со следующими основными типами ресурсов.

\begin{itemize}
	\item \textbf{Pod (под)} описывает экземпляр приложения. В рамках одного пода может быть запущено несколько связанных контейнеров. Является минимальной единицей планирования в Kubernetes, то есть все контейнеры одного пода обязательно будут запущены на одном узле кластера~\cite{pods}.
	\item \textbf{ReplicaSet (набор реплик)} описывает набор реплик запускаемого приложения. На основе него Kubernetes создает поды в количестве, указанном в свойствах объекта~\cite{rs}.
	\item \textbf{Deployment} описывает приложение без сохранения состояния. На основе него Kubernetes создает набор реплик и регулирует процесс обновления этого набора~\cite{deploy}.
	\item \textbf{PersistentVolume (том)} описывает том постоянной области данных, это может быть область диска узла кластера, сетевое хранилище, и так далее~\cite{pv}. Так как том постоянно области данных может быть реализован по-разному, в Kubernetes существует спецификация Container Storage Interface, через которую оркестратор создает тома. Производители оборудования и разработчики могут создавать собственные реализации CSI~\cite{csi}.
	\item \textbf{PersistentVolumeClaim (запрос на том)} описывает пользовательский запрос на том постоянного хранилища данных. Пользователь кластера, если ему необходимо постоянно хранить данные не взаимодействует напрямую с PersistentVolume, а создает запрос на том~\cite{pv}.
	\item \textbf{StatefulSet} описывает приложение с сохранением состояния. На основе него Kubernetes создает запросы на тома и поды в количестве, указанном в спецификации объекта. Каждый под будет использовать уникальный запрос на том~\cite{sts}.
\end{itemize}

Для описания и отображения объектов пользователю используются манифесты. Манифест~---~текстовое описание объекта в Kubernetes в формате YAML~\cite{kubernetesresources}. Пример манифеста пода представлен в листинге~\ref{lst:manifest}.

\begin{lstlisting}[label=lst:manifest, caption={Пример манифеста пода}]
apiVersion: v1
kind: Pod
metadata:
  name: frontend
spec:
  containers:
  - name: app
    image: images.my-company.example/app:v4
    resources:
      requests:
        memory: "64Mi"
        cpu: "250m"
      limits:
        memory: "128Mi"
        cpu: "500m"
  - name: log-aggregator
    image: images.my-company.example/log-aggregator:v6
    resources:
      requests:
        memory: "64Mi"
        cpu: "250m"
      limits:
        memory: "128Mi"
        cpu: "500m"
\end{lstlisting}

В разделе \textbf{spec} содержится спецификация пода (экземпляра приложения). Спецификация пода содержит~\cite{pods}:

\begin{itemize}
\item \textbf{containers}~---~спецификации контейнеров, запускаемых в этом поде, для каждого контейнера могут быть указаны запросы и ограничения на ресурсы (resources), используемые тома (volumes), названия образа Docker (image), название (name);
\item \textbf{volumes}~---~используемые контейнерами тома;
\item \textbf{tolerations}~---~допущения к рабочим узлам. Например, если рабочий узел содержит в спецификации ограничение (taint) вида \textbf{network=low}, то на нем могут быть запущены только те поды, в спецификации которых есть допущение к этому свойству~\cite{taints};
\item \textbf{affinity}~---~правила, влияющие на планирование подов, например, запрет планирования подов приложения на одних узлах с другими экземплярами этого приложения.
\end{itemize}


Система оркестрации контейнеров Kubernetes содержит следующие модули, поставляемые в формате независимых исполняемых файлов~\cite{kubearch}.

\begin{itemize}
	\item \textbf{kube-apiserver}~---~сервис, представляющий Kubernetes API. Работа со всеми объектами, хранящимися в Kubernetes, происходит посредством запросов в \textbf{kube-apiserver}. Кроме информации о всех объектах, сервис управляет информацией о состояниях этих объектов: статусах работы контейнеров, доступными ресурсами узлов кластера, состояниях узлов кластера и пр.
	\item \textbf{etcd}~---~распределенная база данных формата ключ-значение, в которой \textbf{kube-apiserver} хранит информацию о всех объектах кластера~\cite{etcd}.
	\item \textbf{kube-scheduler}~---~планировщик Kubernetes. При создании нового пода, получает информацию о запросах на ресурсы, о свободных ресурсах узлов из \textbf{kube-apiserver}, определяет, на каком узле будет запущено приложение.
	\item \textbf{kube-controller-manager}~---~приложение, обрабатывающее такие ресурсы Kubernetes, как \textbf{Deployment}, \textbf{ReplicaSet} и другие. При создании этих объектов, получает информацию о них в \textbf{kube-apiserver}, и производит соответствующую обработку. Например, для \textbf{ReplicaSet} создаст заданное в спецификации объекта количество подов.
	\item \textbf{kubelet}~---~демон, запускаемый на каждом рабочем узле кластера Kubernetes. Запускает контейнеры, относящиеся к подам, запланированным для запуска на данном узле~\cite{kubelet}.
\end{itemize}

\section{Планирование запуска приложений в Kubernetes}

За планирование запуска приложений в Kubernetes отвечает компонент \textbf{kube-scheduler}. При создании нового пода, под переходит в состояние \textbf{Pending}. Поды в этом состоянии из \textbf{kube-apiserver} получает \textbf{kube-scheduler}, и на основании заданных правил и доступных ресурсов на рабочих узлах, информацию о которых планировщик также получает из \textbf{kube-scheduler}, назначает под на подходящий рабочий узел кластера~\cite{schduler}.

Планирование приложений состоит из двух этапов~\cite{carrion2022kubernetes}\cite{rejiba2022custom}: фильтрация и ранжирование. 

При фильтрации из потенциально подходящих рабочих узлом исключаются те, которые не удовлетворяют заданным условиям, которые иногда называют предикатами~\cite{rejiba2022custom}. Например, \textbf{kube-scheduler} поддерживает следующие модули, реализующие проверки предикатов~\cite{scheduler_profiles}.

\begin{itemize}
	\item \textbf{TaintToleration}~---~допустимыми считаются те узлы, которые не содержат таких ограничений, допущений к которым нет в спецификации подов. 
	\item \textbf{NodeName}~---~допустимым считается только тот узел, имя которого задано в спецификации пода.
	\item \textbf{NodePorts}~---~допустимыми считаются только те узлы, у которых доступны указанные порты.
	\item \textbf{NodeAffinity}~---~допустимыми считаются только те узлы, которые удовлетворяют условиям, заданным в спецификации пода.
	\item \textbf{PodTopologySpread}~---~допустимость узлов определяется географическим расположением узлов.
	\item \textbf{NodeUnschedulable}~---~допустимыми считаются узлы, доступные для планирования.
	\item \textbf{NodeResourcesFit}~---~допустимыми считаются те узлы, каждый из доступных ресурсов которых больше соответствующих запрашиваемых ресурсов пода.
	\item \textbf{VolumeBinding}~---~допустимыми считаются те узлы, которые содержат нужные поду тома данных.
	\item \textbf{VolumeZone}~---~допустимыми считаются те узлы, на которых есть тома, расположенные в нужной геозоне.
	\item \textbf{NodeVolumeLimits}~---~допустимыми считаются те узлы, на которых достаточное количество ресурсов по хранению данных.
	\item \textbf{InterPodAffinity}~---~допустимость узлов определяется, исходя их расположения экземпляров приложения друг относительно друга. Например, у экземпляров приложения может быть установлено правило, что они не могут быть запущены на одном узле. В этом случае допустимыми будут считаться те узлы, на которых не запущены экземпляры указанного приложения.
\end{itemize}

Фильтрация рассчитана на отсечение заведомо неподходящих узлов, поэтому их нет смысла как-либо оптимизировать: нельзя изменить фильтрацию так, чтобы допустимыми считались узлы, которые не подходят для запуска приложения.

Основные исследования в области распределения ресурсов связаны с задачей ранжирования~\cite{senjab2023survey}. При ранжировании происходит приоритизация наиболее подходящих узлов. Далее будут рассмотрены методы, связанные с ранжированием подходящих для выполнения приложения узлов.

\section{Методы планирования запуска приложений}

Подразумевается, что все нижеперечисленные методы могут быть реализованы, или уже реализованы в планировщике Kubernetes. Планировщик представляет собой отдельный сервис, который получает информацию об ожидающих планирования подах и свободных ресурсах на рабочих узлах из \textbf{kube-apiserver}.

\subsection{Метод случайного распределения}

С использованием данного метода, в качестве рабочего узла для выполнения задачи выбирается случайный узел~\cite{psychas2018randomized}. Так как планировщик получает из \textbf{kube-apiserver} список рабочих узлов кластера, затем фильтрует его, то из списка оставшихся потенциально подходящих узлов можно выбрать номер узла с использованием генератора псевдослучайных чисел. Так как на вход алгоритму ранжирования не могут поступить узлы, на которых под не может быть запущен, можно использовать любой метод генерации псевдослучайных чисел~\cite{wen2023k8ssim}.

К данной категории также можно отнести метод «Round Robin»~\cite{singh2010optimized}: для задачи выбирается следующий доступный в кольцевой очереди узел. 

Данные методы нельзя в полной мере считать подходящими для целевой задачи, так как они не учитывают распределение ресурсов, текущую нагрузку на узлах, запросы задачи на ресурсы. 

\subsection{Выбор наименее запрашиваемого узла}

%%% Proposal: Исследовать скорость работы планировщика (на будущее)

В качестве подходящего рабочего узла выбирается наименее запрашиваемый узел~\cite{liu2010performance}\cite{wen2023k8ssim}. Данный метод не учитывает распределение ресурсов, текущую нагрузку на узлах, запросы задачи на ресурсы. При этом, за счет выбора наименее запрашиваемого ресурса в некоторых случаях позволяет равномерно распределить нагрузку между рабочими узлами, например в том случае, когда узлы имеют одинаковые ресурсы, а задачи одинаковые запросы на ресурсы~\cite{beltre2019kubesphere}.

\subsection{Выбор наиболее запрашиваемого узла}

В качестве подходящего рабочего узла выбирается наиболее запрашиваемый узел~\cite{liu2010performance}. В отличие от метода выбора наименее запрашиваемого узла, такой метод не распределяет нагрузку равномерно: узлы заполняются задачами «по очереди», задачи начнут запускаться на узле только в том случае, когда остальные узлы максимально загружены и не могут принять ее. При этом, метод также не учитывает распределение ресурсов, текущую нагрузку на узлах, запросы задачи на ресурсы.

\subsection{Метод сбалансированного распределения}

Для выбора подходящего узла рассчитывается коэффициент загрузки задачей свободных ресурсов для каждого узла~\cite{ghit2014balanced}\cite{kubernetesbalsched}:

\begin{equation}
	Load_{ij} = \frac{Requests_{j}}{Available_{ij}},
\end{equation}

где 

\begin{itemize}
	\item $K$~---~количество системных ресурсов;
	\item $N$~---~количество доступных рабочих узлов;
	\item $Load_{N \times K}$~---~матрица, в которой содержатся потенциальные коэффициенты загрузки рабочего узла задачей для каждого вида ресурсов, строка соответствует рабочему узлу, столбец~---~ресурсу;
	\item $Requests_{K}$~---~вектор, содержащий запросы на ресурсы приложением;
	\item $Available_{N \times K}$~---~матрица, в которой содержатся доступные ресурсы рабочих узлов, строка соответствует рабочему узлу, столбец~---~ресурсу;
	\item $i$~---~номер рабочего узла;
	\item $j$~---~номер ресурса.
\end{itemize}

Зачем рассчитывается вектор, содержащий средние показатели загрузки каждого узла:

\begin{equation}
	Mean_{i} = \frac{\sum_{j = 1}^{K} Load_{ij}}{K}.
\end{equation}

На основе средних показателей загрузки можно рассчитать оценку для каждого узла:

\begin{equation}
	Score_{i} = \sqrt{\frac{\sum_{j = 1}^{K} (Load_{ij} - Mean_{i})^2}{K}}.
\end{equation}

Подходящим узлом считается узел с наибольшей оценкой. Таким образом, метод учитывает текущую загрузку узлов и запросы на ресурсы задачей. Данный метод стремится равномерно распределить ресурсы узлов, например, таким образом, чтобы процессор и память на узлах были загружены одинаково.

Данный метод используется в Kubernetes по умолчанию~\cite{kubernetesbalsched}.

\subsection{Метод сбалансированного распределения по доминантному ресурсу}

На практике вышеизложенный метод может приводить к неравномерной нагрузке рабочих узлов~\cite{el2021kubcg}: на одном узле может быть полностью загружен процессор, а на другом~---~память. 

Например, в кластере, состоящем из трех серверов:

\begin{enumerate}
	\item \textit{server 1}~---~CPU: 6000 единиц процессорного времени, Memory: 4.5ГБ;
	\item \textit{server 2}~---~CPU: 3000 единиц процессорного времени, Memory: 3ГБ;
	\item \textit{server 3}~---~CPU: 3000 единиц процессорного времени, Memory: 6ГБ;
\end{enumerate}

\noindent пользователь пытается запустить семь подов в указанном порядке:

\begin{enumerate}
	\item \textit{приложение 1}~---~запрашивает CPU: 2500 единиц процессорного времени, Memory: 1ГБ;
	\item \textit{приложение 2}~---~запрашивает CPU: 1000 единиц процессорного времени, Memory: 2.5ГБ;
	\item \textit{приложение 3}~---~запрашивает CPU: 2000 единиц процессорного времени, Memory: 1ГБ;
	\item \textit{приложение 4}~---~запрашивает CPU: 1000 единиц процессорного времени, Memory: 2.5ГБ;
	\item \textit{приложение 5}~---~запрашивает CPU: 2000 единиц процессорного времени, Memory: 1ГБ;
	\item \textit{приложение 6}~---~запрашивает CPU: 1000 единиц процессорного времени, Memory: 2ГБ;
	\item \textit{приложение 7}~---~запрашивает CPU: 1000 единиц процессорного времени, Memory: 2.5ГБ.
\end{enumerate}

Результат работы метода сбалансированного распределения проиллюстрирован на рисунке~\ref{img:balanced}, приложения отмечены цветными фигурами, где высота фигуры~---~потребление ресурса системы, цифры~---~порядковые номера, в которых запускались приложения.

\newpage

\includeimage
    {balanced}
    {f}
    {h}
    {0.8\textwidth}
    {Возможное распределение приложений по серверам при использовании метода сбалансированного распределения}
    
Видно, что приложение №7 не может быть запущено в кластере, так как на всех серверах недостаточно свободных ресурсов. При этом, сервера имеют следующие относительные показатели загрузки, округленные до целых:

\begin{enumerate}
	\item \textit{server 1}~---~CPU: 92\%, Memory: 100\%;
	\item \textit{server 2}~---~CPU: 33\%, Memory: 83\%;
	\item \textit{server 3}~---~CPU: 100\%, Memory: 50\%.
\end{enumerate}

Для того, чтобы решить эту проблему, существует модификация этого метода. Его отличие заключается в том, что для оценки каждого узла учитывается только доминантный ресурс, то есть оценка узлов происходит на основе коэффициента $Dominant$~\cite{wang2014dominant}, расчитываемого для каждого узла $i$:

\begin{equation}
	Dominant_{i} = \max_{j = 1}^{K} Load{ij}.
\end{equation}

Подходящим считается узел с наименьшим $Dominant$. Иллюстрация работы данного метода приведена на рисунке~\ref{img:dominant}.

\includeimage
    {dominant}
    {f}
    {h}
    {0.8\textwidth}
    {Возможное распределение приложений по серверам при использовании метода сбалансированного распределения по доминантному ресурсу}
    
В данном примере, приложение №7 было запущено, а сервера имеют следующие относительные показатели загрузки, округленные до целых:

\begin{enumerate}
	\item \textit{server 1}~---~CPU: 92\%, Memory: 100\%;
	\item \textit{server 2}~---~CPU: 100\%, Memory: 100\%;
	\item \textit{server 3}~---~CPU: 67\%, Memory: 83\%.
\end{enumerate}

\subsection{Генетический алгоритм}

Генетический алгоритмы~---~поисковые алгоритмы, основанные на механизмах натуральной селекции и натуральной генетики~\cite{курейчик1998генетические}. Случайным образом выбирается какое-либо решение, далее происходит череда мутаций, в процессе которых решение незначительно изменяется с использованием элементарных операций. Полученное решение проверяется на предмет допустимости. В результате многочисленных мутаций выбирается наиболее оптимальное решение.

Важно отметить, что генетические алгоритмы обычно используются при большом количестве учитываемых параметров~\cite{гладков2022генетические}. В обозреваемой задаче количество параметров можно считать небольшим:

\begin{enumerate}
	\item максимальное количество рабочих узлов в кластере Kubernetes~---~5000~\cite{large}, однако на практике количество рабочих узлов может быть ограничено меньшими значениями, например, 512~\cite{digitalocean};
	\item в стандартном случае при планировании подов учитываются два системных ресурса~---~процессор и память~\cite{resource_management}, однако пользователи кластера могут задавать пользовательские типы системных ресурсов, например, пропускную способность дискового ввода-вывода~\cite{phdays}.
\end{enumerate}

\subsection{Нейронные сети}

Нейронные сети — вычислительные системы или машины, созданные для моделирования аналитических действий, совершаемых человеческим мозгом.

Нейронные сети относятся к направлению искусственного интеллекта (ИИ) и применяются для распознавания скрытых закономерностей в необработанных данных, группировки и классификации, а также решения задач в области ИИ, машинного и глубокого обучения.

Искусственные нейронные сети состоят из нескольких слоев: входных, скрытых, выходных. В каждом из них есть несколько узлов, которые соединены со всеми узлами в сети с помощью разных связей и имеют свой «вес», влияющий на силу передаваемого сигнала. Такая архитектура позволяет вести параллельную обработку данных и постоянно сравнивать их с результатами обработки на каждом из этапов. Нейронные сети изначально обучаются на размеченных наборах данных с очевидными закономерностями, а после используют полученные навыки для самообучения и достижения результата. При этом нейросеть может совершать миллионы попыток для достижения таких же результатов, как и предоставленном для обучения примере~\cite{neural}.

В данном случае, входными данными для нейронной сети будут запросы на ресурсы приложения, и доступные ресурсы рабочих узлов. Результатом работы нейронной сети будет номер наиболее подходящего узла.

Важно отметить, что для обучения нейронной сети необходима большая выборка~\cite{elman1993learning}, которых на данных момент не существует в открытом доступе для указанной задачи. Кроме того, тестировать и отлаживать компоненты, использующие нейронные сети, крайне сложно~\cite{sun2018testing}.

\subsection{Метод выселения}

Так как такие системы, как Kubernetes, являются очень динамичными, то может возникнуть ситуация, когда в какой-то момент времени уже существующее распределение задач по узлам может оказаться неоптимальным. Например, когда не хватает ресурсов для запуска какого-либо приложения, при этом если перераспределить задачи по-другому, то задача может быть запущена~\cite{descheduler}. Для таких случаев существует метод выселения, который заключается в том, что при обнаружении неоптимального распределения задач в кластере, происходит их перераспределение. В некоторых случаях использование выселения позволяет сократить количество использованных узлов для запуска приложений на 16.7\%~\cite{larsson2023impact}.

Особенность этого метода заключается в том, что он не может быть бесконтрольно применяем в рамках кластера, так как бесконтрольные выселения приложений, по сути представляющие собой перезапуски, могут негативно влиять на доступность системы. 

Кроме того, для баз данных, перезапуск одной из реплик, в случае недоступности нескольких других реплик, может привести к недоступности всей базы данных~\cite{ongaro2015raft}.

